{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Pipeline\n",
    "**Please review the code cell below and enter the path for your model .pt and .bin files, and a directory where output should be saved**. Then just run the notebook.\n",
    "\n",
    "Look for the .txt and .output files in your working directly. Those are the results.\n",
    "\n",
    "This pipeline is heavily reliant on other functions scattered throughout the repo, so please be sure you haven't moved it anywhere else. It is also heavily reliant on the model structure and output files generated during training with our base code. It is finicky enough that if you have any issues, it's probably easiest to just ask me to fix it (or at least show me the error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the directory where you would like final result files to be saved.\n",
    "# Then enter the path for your model .pt and .bin files. You must have both.\n",
    "WORKING_DIR = 'C:/Users/eweeding/Documents/MSE/DeepLearning/FinalProject'\n",
    "MODEL_PT = WORKING_DIR+'/model_mini.pt'\n",
    "MODEL_BIN = WORKING_DIR+'/model_mini.bin'\n",
    "\n",
    "# Set analysis mode. Options are 'overall' (default) or 'full'.\n",
    "MODE = 'overall'\n",
    "#MODE = 'full'\n",
    "\n",
    "# If ANIM == True, examine the effects of animacy on the results. Default is False.\n",
    "ANIM = False\n",
    "\n",
    "### OPTIONAL ###\n",
    "# Other paths you may need or want to change depending on your setup:\n",
    "TEMPLATE_DIR = './EMNLP2018/templates' # Location where the paired sentence dataset should be\n",
    "OUTPUT_FILE = 'all_test_sents.txt'     # Name of .txt file which will contain all test sentences, saved in TEMPLATE_DIR\n",
    "MODEL_TYPE = 'RNN'                     # Model type. For now, only single-task RNN is supported, so don't change this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "import logging\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from src.tester.TestWriter import TestWriter\n",
    "from src.template.TestCases import TestCase\n",
    "\n",
    "# Import a module within the base code\n",
    "# Pathing workaround due to hyphens in folder name\n",
    "sys.path.append(os.path.join(os.getcwd(), 'word-language-model'))\n",
    "import data\n",
    "\n",
    "# Suppress some intermediate messages\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "# Check for GPU\n",
    "gpu_avail = torch.cuda.is_available()\n",
    "print('GPU available:', gpu_avail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate paired sentences\n",
    "This generates the sentences for testing. These already exist in `./EMNLP2018/templates`, but you may need to generate them yourself if Pickle is behaving strangely with the downloaded files based on your OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case: obj_rel_across_anim\n",
      "case: obj_rel_within_anim\n",
      "case: obj_rel_across_inanim\n",
      "case: obj_rel_within_inanim\n",
      "case: subj_rel\n",
      "case: prep_anim\n",
      "case: prep_inanim\n",
      "case: obj_rel_no_comp_across_anim\n",
      "case: obj_rel_no_comp_within_anim\n",
      "case: obj_rel_no_comp_across_inanim\n",
      "case: obj_rel_no_comp_within_inanim\n",
      "case: simple_agrmt\n",
      "case: sent_comp\n",
      "case: vp_coord\n",
      "case: long_vp_coord\n",
      "case: reflexives_across\n",
      "case: simple_reflexives\n",
      "case: reflexive_sent_comp\n",
      "case: npi_across_anim\n",
      "case: npi_across_inanim\n",
      "case: simple_npi_anim\n",
      "case: simple_npi_inanim\n"
     ]
    }
   ],
   "source": [
    "%run ./src/make_templates.py $TEMPLATE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test a model on the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for calculating complexity measures\n",
    "def get_entropy(o):\n",
    "    probs = nn.functional.softmax(o,dim=0)\n",
    "    logprobs = nn.functional.log_softmax(o,dim=0)\n",
    "    return -1 * torch.sum(probs * logprobs)\n",
    "\n",
    "\n",
    "def get_surps(o):\n",
    "    logprobs = nn.functional.log_softmax(o,dim=0)\n",
    "    return -1 * logprobs\n",
    "\n",
    "\n",
    "def get_complexity_apply(o,t,sentid,corpus,tags=False):\n",
    "    Hs = torch.squeeze(apply(get_entropy,o))\n",
    "    surps = apply(get_surps,o)    \n",
    "    for corpuspos, targ in enumerate(t):\n",
    "        if tags:\n",
    "            word = corpus.dictionary.idx2tag[int(targ)]\n",
    "        else:\n",
    "            word = corpus.dictionary.idx2word[int(targ)]\n",
    "        if word == '<eos>' or word == '<EOS>':\n",
    "            # Don't output the complexity of EOS\n",
    "            continue\n",
    "        surp = surps[corpuspos][int(targ)]        \n",
    "        with open(WORKING_DIR+'/RNN.output', 'a') as f:\n",
    "            f.write('\\n' + str(word)+' '+str(sentid)+' '+str(corpuspos)+' '+str(len(word))+' '+str(float(surp))+' '+str(float(Hs[corpuspos])))\n",
    "\n",
    "            \n",
    "def apply(func, M):\n",
    "    tList = [func(m) for m in torch.unbind(M,dim=0) ]\n",
    "    res = torch.stack(tList)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core testing functions\n",
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)  \n",
    "    \n",
    "    \n",
    "def test_get_batch(source, evaluation=False):\n",
    "    if isinstance(source, tuple):\n",
    "        seq_len = len(source[0]) - 1\n",
    "        data = Variable(source[0][:seq_len], requires_grad=False)\n",
    "        target = Variable(source[1][:seq_len], requires_grad=False)\n",
    "    else:\n",
    "        seq_len = len(source) - 1\n",
    "        data = Variable(source[:seq_len], requires_grad=False)\n",
    "        target = Variable(source[1:1+seq_len].view(-1))\n",
    "    if gpu_avail:\n",
    "        return data.cuda(), target.cuda()\n",
    "    else:\n",
    "        return data, target\n",
    "\n",
    "    \n",
    "def test_evaluate(test_lm_sentences, test_ccg_sentences, lm_data_source, ccg_data_source, model, corpus):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()    \n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    \n",
    "    with open(WORKING_DIR+'/RNN.output', 'w') as f:\n",
    "        f.write('word sentid sentpos wlen surp entropy')    \n",
    "\n",
    "    for i in range(len(lm_data_source)+len(ccg_data_source)):\n",
    "        if i % 1000 == 0:\n",
    "            print(f'{i} / {len(lm_data_source)} sentences')\n",
    "            \n",
    "        if i >= len(lm_data_source):\n",
    "            sent_ids = ccg_data_source[i-len(lm_data_source)]\n",
    "            sent = test_ccg_sentences[i-len(lm_data_source)]\n",
    "        else:\n",
    "            sent_ids = lm_data_source[i]\n",
    "            sent = test_lm_sentences[i]\n",
    "            \n",
    "        if gpu_avail:\n",
    "            sent_ids = sent_ids.cuda()\n",
    "            \n",
    "        hidden = model.init_hidden(1)\n",
    "        data, targets = test_get_batch(sent_ids, evaluation=True)\n",
    "        data = data.unsqueeze(1)\n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        curr_loss = criterion(output_flat, targets).item()\n",
    "        total_loss += curr_loss\n",
    "\n",
    "        # Get word-level complexity metrics\n",
    "        if i >= len(lm_data_source):\n",
    "            get_complexity_apply(output_flat,targets,i-len(lm_data_source),corpus,tags=True)\n",
    "        else:\n",
    "            get_complexity_apply(output_flat,targets,i,corpus)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "\n",
    "    return total_loss / (len(lm_data_source)+len(ccg_data_source))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A main function that coordinates testing\n",
    "def run_main(save, save_lm_data, lm_data, testfname):\n",
    "    corpus = data.SentenceCorpus(lm_data, False, save_lm_data, True, testfname=testfname)\n",
    "    test_lm_sentences, test_lm_data = corpus.test_lm\n",
    "    test_ccg_sentences = []\n",
    "    test_ccg_data = []    \n",
    "\n",
    "    # Load the saved model\n",
    "    if gpu_avail:\n",
    "        model = torch.load(save, map_location = 'cuda:0')\n",
    "    else:\n",
    "        model = torch.load(save, map_location = 'cpu')    \n",
    "    print('Your model is:')\n",
    "    print(model)    \n",
    "\n",
    "    # Run on test data\n",
    "    test_loss = test_evaluate(test_lm_sentences, test_ccg_sentences, test_lm_data, test_ccg_data, model, corpus)\n",
    "    print('=' * 89)\n",
    "    print('| End of Testing | Test Loss {:5.2f} | Test PPL {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "    print('=' * 89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final functions to combine model testing and scoring on sentences\n",
    "def score_rnn():\n",
    "    print(\"Scoring RNN...\")\n",
    "    with open(WORKING_DIR+'/rnn.output', 'r') as f:\n",
    "        all_scores = {}\n",
    "        first = False\n",
    "        score = 0.\n",
    "        sent = []\n",
    "        prev_sentid = -1\n",
    "        for line in f:\n",
    "            if not first:\n",
    "                first = True\n",
    "                continue\n",
    "            if first and len(line.strip().split()) == 6 and \"torch.cuda\" not in line:\n",
    "                wrd, sentid, wrd_score = [line.strip().split()[i] for i in [0,1,4]]\n",
    "                score = -1 * float(wrd_score) # multiply by -1 to turn surps back into logprobs\n",
    "                sent.append((wrd, score))\n",
    "                if wrd == \".\":\n",
    "                    name_found = False                    \n",
    "                    for (k1,v1) in sorted(name_lengths.items(), key=operator.itemgetter(1)):\n",
    "                        if float(sentid) < v1 and not name_found:\n",
    "                            name_found = True\n",
    "                            if k1 not in all_scores:\n",
    "                                all_scores[k1] = {}\n",
    "                            key_found = False\n",
    "                            for (k2,v2) in sorted(key_lengths[k1].items(), key=operator.itemgetter(1)):\n",
    "                                if int(sentid) <  v2 and not key_found:\n",
    "                                    key_found = True\n",
    "                                    if k2 not in all_scores[k1]:\n",
    "                                        all_scores[k1][k2] = []\n",
    "                                    all_scores[k1][k2].append(sent)\n",
    "                    sent = []\n",
    "                    prev_sentid = float(sentid)\n",
    "    return all_scores\n",
    "\n",
    "\n",
    "def test_LM():      \n",
    "    print(\"Testing RNN...\")\n",
    "    run_main(MODEL_PT, MODEL_BIN, TEMPLATE_DIR, OUTPUT_FILE)\n",
    "    results = score_rnn()\n",
    "    with open(WORKING_DIR+'/'+MODEL_TYPE+'_results.pickle', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print('Scoring done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RNN...\n",
      "Your model is:\n",
      "RNNModel(\n",
      "  (drop): Dropout(p=0.2, inplace=False)\n",
      "  (encoder): Embedding(50001, 200)\n",
      "  (rnn): LSTM(200, 200, num_layers=2, dropout=0.2)\n",
      "  (decoder): Linear(in_features=200, out_features=50001, bias=True)\n",
      ")\n",
      "0 / 11916 sentences\n",
      "1000 / 11916 sentences\n",
      "2000 / 11916 sentences\n",
      "3000 / 11916 sentences\n",
      "4000 / 11916 sentences\n",
      "5000 / 11916 sentences\n",
      "6000 / 11916 sentences\n",
      "7000 / 11916 sentences\n",
      "8000 / 11916 sentences\n",
      "9000 / 11916 sentences\n",
      "10000 / 11916 sentences\n",
      "11000 / 11916 sentences\n",
      "=========================================================================================\n",
      "| End of Testing | Test Loss  7.12 | Test PPL  1236.42\n",
      "=========================================================================================\n",
      "Scoring RNN...\n",
      "Scoring done.\n"
     ]
    }
   ],
   "source": [
    "# Select constructions to test (automatically selects all)\n",
    "writer = TestWriter(TEMPLATE_DIR, OUTPUT_FILE)\n",
    "testcase = TestCase()\n",
    "tests = testcase.all_cases\n",
    "all_test_sents = {}\n",
    "for test_name in tests:\n",
    "    test_sents = pickle.load(open(TEMPLATE_DIR+\"/\"+test_name+\".pickle\", 'rb'))\n",
    "    all_test_sents[test_name] = test_sents\n",
    "writer.write_tests(all_test_sents, 'word')\n",
    "name_lengths = writer.name_lengths\n",
    "key_lengths = writer.key_lengths\n",
    "\n",
    "# Finally, proceed with testing\n",
    "test_LM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for higher level interpretation of model results\n",
    "def is_more_probable(sent_a, sent_b):\n",
    "    if len(sent_a) != len(sent_b):\n",
    "        print(\"ERROR: Mismatch in sentence lengths: (1) \"+sent_a+\" vs (2) \"+sent_b)\n",
    "    return sum([sent_a[i][1] for i in range(len(sent_a))]) > sum([sent_b[i][1] for i in range(len(sent_b))])\n",
    "\n",
    "\n",
    "def analyze_agrmt_results(results):\n",
    "    correct_sents = {}\n",
    "    incorrect_sents = {}\n",
    "    for case in results.keys():\n",
    "        correct_sents[case] = []\n",
    "        incorrect_sents[case] = []\n",
    "        for i in range(0,len(results[case]),2):\n",
    "            grammatical = results[case][i]\n",
    "            ungrammatical = results[case][i+1]\n",
    "            if is_more_probable(grammatical, ungrammatical):\n",
    "                correct_sents[case].append((grammatical, ungrammatical))\n",
    "            else:\n",
    "                incorrect_sents[case].append((grammatical, ungrammatical))\n",
    "    return correct_sents, incorrect_sents\n",
    "\n",
    "\n",
    "def analyze_npi_results(results):\n",
    "    options = ['gi_g', 'gi_i','iu_i','iu_u','gu_g','gu_u']\n",
    "    sentences = {}\n",
    "    for opt in options:\n",
    "        sentences[opt] = {}\n",
    "        for case in results.keys():\n",
    "            sentences[opt][case] = []\n",
    "    for case in results.keys():\n",
    "        for i in range(0,len(results[case]),3):\n",
    "            grammatical = results[case][i]\n",
    "            intrusive = results[case][i+1]\n",
    "            ungrammatical = results[case][i+2]\n",
    "            g_sent = grammatical\n",
    "            i_sent = intrusive\n",
    "            u_sent = ungrammatical\n",
    "            if is_more_probable(grammatical, intrusive):\n",
    "                sentences['gi_g'][case].append((g_sent, i_sent, u_sent))\n",
    "            else:\n",
    "                sentences['gi_i'][case].append((g_sent, i_sent, u_sent))\n",
    "            if is_more_probable(grammatical, ungrammatical):\n",
    "                sentences['gu_g'][case].append((g_sent, i_sent, u_sent))\n",
    "            else:\n",
    "                sentences['gu_u'][case].append((g_sent, i_sent, u_sent))\n",
    "            if is_more_probable(intrusive, ungrammatical):\n",
    "                sentences['iu_i'][case].append((g_sent, i_sent, u_sent))\n",
    "            else:\n",
    "                sentences['iu_u'][case].append((g_sent, i_sent, u_sent))\n",
    "    return [sentences[x] for x in options]\n",
    "\n",
    "\n",
    "def display_agrmt_results(name, sents):\n",
    "    correct_sents, incorrect_sents = sents\n",
    "    overall_correct = 0.\n",
    "    total = 0.\n",
    "    strings = {}\n",
    "    case_accs = {}\n",
    "    for case in correct_sents.keys():\n",
    "        if MODE == 'full':\n",
    "            string = \"\"\n",
    "            if len(correct_sents[case]) > 0:\n",
    "                for i in range(len(correct_sents[case][0][0])):\n",
    "                    if correct_sents[case][0][0][i][0] == correct_sents[case][0][1][i][0]:\n",
    "                        string += correct_sents[case][0][0][i][0] + \" \"\n",
    "                    else: string += correct_sents[case][0][0][i][0]+\"/*\"+correct_sents[case][0][1][i][0]+\" \"\n",
    "            else:\n",
    "                for i in range(len(incorrect_sents[case][0][0])):\n",
    "                    if incorrect_sents[case][0][0][i][0] == incorrect_sents[case][0][1][i][0]:\n",
    "                        string += incorrect_sents[case][0][0][i][0] + \" \"\n",
    "                    else: string += incorrect_sents[case][0][0][i][0]+\"/*\"+incorrect_sents[case][0][1][i][0]+\" \"\n",
    "            strings[case] = string[:-1]\n",
    "            case_accs[case] = float(len(correct_sents[case]))/(len(correct_sents[case])+len(incorrect_sents[case]))\n",
    "        overall_correct += len(correct_sents[case])\n",
    "        total += len(correct_sents[case]) + len(incorrect_sents[case])\n",
    "        \n",
    "    # Case-by-case accuracies\n",
    "    if MODE == 'full':\n",
    "        case_out = open(WORKING_DIR+'/case_accs.txt', 'w')\n",
    "        case_out.write(\"\\n##########\\n\" + name + \"\\n##########\\n\"+\"Overall acc: \"+str(float(overall_correct)/total)+\"\\n\")\n",
    "        for (case,score) in sorted(case_accs.items(), key=operator.itemgetter(1)):\n",
    "            case_out.write(str(case)+\":\\n\")\n",
    "            case_out.write(strings[case]+\": \"+str(round(score,4))+\"\\n\")\n",
    "        case_out.write(\"\\n\")\n",
    "        case_out.close()\n",
    "        \n",
    "    # Individual scores\n",
    "    if MODE == 'full':\n",
    "        fout = open(WORKING_DIR+'/individual_accs.txt', 'w')\n",
    "        fout.write(\"\\n##########\\n\" + name + \"\\n##########\\n\\n\")\n",
    "        fout.write(\"Examples that the LM predicts incorrectly:\\n\")\n",
    "        for case in incorrect_sents.keys():\n",
    "            count = 0\n",
    "            fout.write(case+\":\\n\")\n",
    "            for good, bad in incorrect_sents[case]:\n",
    "                if count < 5:\n",
    "                    fout.write(\"Grammatical:\"+str(round(sum([x[1] for x in good]),2))+\"\\n\")\n",
    "                    fout.write('\\t'.join([x[0] for x in good])+\"\\n\")\n",
    "                    fout.write('\\t'.join([str(round(x[1],2)) for x in good])+\"\\n\")\n",
    "                    fout.write(\"Ungrammatical:\"+str(round(sum([x[1] for x in bad]),2))+\"\\n\")\n",
    "                    fout.write('\\t'.join([x[0] for x in bad])+\"\\n\")\n",
    "                    fout.write('\\t'.join([str(round(x[1],2)) for x in bad])+\"\\n\")\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "        fout.write(\"\\n\")\n",
    "        fout.close()\n",
    "    return float(overall_correct)/total\n",
    "\n",
    "\n",
    "def display_npi_results(name, sents):\n",
    "    gi_grammatical_sents, gi_intrusive_sents, iu_intrusive_sents, iu_ungrammatical_sents, gu_grammatical_sents, gu_ungrammatical_sents = sents\n",
    "    overall_gi = 0.\n",
    "    overall_iu = 0.\n",
    "    overall_gu= 0.\n",
    "    total_gi = 0.\n",
    "    total_iu = 0.\n",
    "    total_gu = 0.\n",
    "    strings = {}\n",
    "    gi_case_accs = {}\n",
    "    iu_case_accs = {}\n",
    "    gu_case_accs = {}\n",
    "    for case in gi_grammatical_sents.keys():\n",
    "        gi_case_accs[case] = {}\n",
    "        iu_case_accs[case] = {}\n",
    "        gu_case_accs[case] = {}        \n",
    "        if MODE == 'full':\n",
    "            if len(gi_grammatical_sents[case]) > 0:\n",
    "                string = ' '.join([x[0] for x in gi_grammatical_sents[case][0][0]]) + \" vs. \" + ' '.join([x[0] for x in gi_grammatical_sents[case][0][1]]) + \" vs. \" + ' '.join([x[0] for x in gi_grammatical_sents[case][0][2]])\n",
    "            else:\n",
    "                string = ' '.join([x[0] for x in gi_intrusive_sents[case][0][0]]) + \" vs. \" + ' '.join([x[0] for x in gi_intrusive_sents[case][0][1]]) + \" vs. \" + ' '.join([x[0] for x in gi_intrusive_sents[case][0][2]])\n",
    "            strings[case] = string\n",
    "            gi_case_accs[case] = float(len(gi_grammatical_sents[case]))/(len(gi_grammatical_sents[case])+len(gi_intrusive_sents[case]))\n",
    "            iu_case_accs[case] = float(len(iu_intrusive_sents[case]))/(len(iu_intrusive_sents[case])+len(iu_ungrammatical_sents[case]))\n",
    "            gu_case_accs[case] = float(len(gu_grammatical_sents[case]))/(len(gu_grammatical_sents[case])+len(gu_ungrammatical_sents[case]))\n",
    "        overall_gi += len(gi_grammatical_sents[case])\n",
    "        overall_iu += len(iu_intrusive_sents[case])\n",
    "        overall_gu += len(gu_grammatical_sents[case])\n",
    "        total_gi += len(gi_grammatical_sents[case]) + len(gi_intrusive_sents[case])\n",
    "        total_iu += len(iu_intrusive_sents[case]) + len(iu_ungrammatical_sents[case])\n",
    "        total_gu += len(gu_grammatical_sents[case]) + len(gu_ungrammatical_sents[case])\n",
    "        \n",
    "    # Case-by-case accuracies\n",
    "    if MODE == 'full':\n",
    "        case_out = open(WORKING_DIR+'/case_accs.txt', 'a')\n",
    "        case_out.write(\"\\n##########\\n\" + name + \"\\n##########\\n\" + \"Overall acc:\\n\")\n",
    "        case_out.write(\"OVERALL P(GRAMMATICAL) > P(INTRUSIVE): \"+str(float(overall_gi)/total_gi)+\"\\n\")\n",
    "        case_out.write(\"OVERALL P(INTRUSIVE) > P(UNGRAMMATICAL): \"+str(float(overall_iu)/total_iu)+\"\\n\")\n",
    "        case_out.write(\"OVERALL P(GRAMMATICAL) > P(UNGRAMMATICAL): \"+str(float(overall_gu)/total_gu)+\"\\n\")\n",
    "        for (case,score) in sorted(gi_case_accs.items(), key=operator.itemgetter(1)):\n",
    "            case_out.write(str(case)+\":\\n\")\n",
    "            case_out.write(strings[case]+\":\\n\")\n",
    "            case_out.write(\"Grammatical > intrusive: \"+str(round(score,4))+\"\\n\")\n",
    "            case_out.write(\"Grammatical > ungrammatical: \"+str(gu_case_accs[case])+\"\\n\")\n",
    "            case_out.write(\"Intrusive > ungrammatical: \"+str(iu_case_accs[case])+\"\\n\")\n",
    "        case_out.write(\"\\n\")\n",
    "        case_out.close()\n",
    "    \n",
    "    # Individual scores\n",
    "    if MODE == 'full':\n",
    "        fout = open(WORKING_DIR+'/individual_accs.txt', 'a')\n",
    "        fout.write(\"\\n##########\\n\" + name + \"\\n##########\\n\\n\")\n",
    "        fout.write(\"Examples where the LM prefers the intrusive licensor over the grammatical case:\\n\")\n",
    "        for case in gi_intrusive_sents.keys():\n",
    "            count = 0\n",
    "            fout.write(case+\":\\n\")\n",
    "            for c,i,u in gi_intrusive_sents[case]:\n",
    "                if count < 5:\n",
    "                    fout.write(\"grammatical:\"+str(round(sum([x[1] for x in c]),2))+\"\\n\")\n",
    "                    fout.write('\\t'.join([x[0] for x in c])+\"\\n\")\n",
    "                    fout.write('\\t'.join([str(round(x[1],2)) for x in c])+\"\\n\")\n",
    "                    fout.write(\"intrusive:\"+str(round(sum([x[1] for x in i]),2))+\"\\n\")\n",
    "                    fout.write('\\t'.join([x[0] for x in i])+\"\\n\")\n",
    "                    fout.write('\\t'.join([str(round(x[1],2)) for x in i])+\"\\n\")\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "        fout.write(\"\\nExamples where the LM prefers the ungrammatical case over the intrusive licensor:\\n\")\n",
    "        for case in iu_ungrammatical_sents.keys():\n",
    "            count = 0\n",
    "            fout.write(case+\":\\n\")\n",
    "            for c,i,u in iu_ungrammatical_sents[case]:\n",
    "                if count < 5:\n",
    "                    fout.write(\"intrusive:\"+str(round(sum([x[1] for x in i]),2))+\"\\n\")\n",
    "                    fout.write('\\t'.join([x[0] for x in i])+\"\\n\")\n",
    "                    fout.write('\\t'.join([str(round(x[1],2)) for x in i])+\"\\n\")\n",
    "                    fout.write(\"ungrammatical:\"+str(round(sum([x[1] for x in u]),2))+\"\\n\")\n",
    "                    fout.write('\\t'.join([x[0] for x in u])+\"\\n\")\n",
    "                    fout.write('\\t'.join([str(round(x[1],2)) for x in u])+\"\\n\")\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "        fout.write(\"\\nExamples where the LM prefers the ungrammatical case over the grammatical case:\\n\")\n",
    "        for case in gu_ungrammatical_sents.keys():\n",
    "            count = 0\n",
    "            fout.write(case+\":\\n\")\n",
    "            for c,i,u in gu_ungrammatical_sents[case]:\n",
    "                if count < 5:\n",
    "                    fout.write(\"grammatical:\"+str(round(sum([x[1] for x in c]),2))+\"\\n\")\n",
    "                    fout.write('\\t'.join([x[0] for x in c])+\"\\n\")\n",
    "                    fout.write('\\t'.join([str(round(x[1],2)) for x in c])+\"\\n\")\n",
    "                    fout.write(\"ungrammatical:\"+str(round(sum([x[1] for x in u]),2))+\"\\n\")\n",
    "                    fout.write('\\t'.join([x[0] for x in u])+\"\\n\")\n",
    "                    fout.write('\\t'.join([str(round(x[1],2)) for x in u])+\"\\n\")\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "        fout.write(\"\\n\")\n",
    "        fout.close()\n",
    "   \n",
    "    return float(overall_gi)/total_gi, float(overall_iu)/total_iu, float(overall_gu)/total_gu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model analysis\n",
    "results = pickle.load(open(WORKING_DIR+'/RNN_results.pickle', 'rb'))\n",
    "\n",
    "# Combine results w.r.t. animacy if desired\n",
    "if not ANIM:\n",
    "    joined_results = {}\n",
    "    for name in tests:\n",
    "        if 'anim' in name:\n",
    "            new_name = '_'.join(name.split(\"_\")[:-1])\n",
    "        else:\n",
    "            new_name = name\n",
    "        for sub_case in results[name]:\n",
    "            if new_name not in joined_results:\n",
    "                joined_results[new_name] = {}\n",
    "            if sub_case not in joined_results[new_name]:\n",
    "                joined_results[new_name][sub_case] = []\n",
    "            joined_results[new_name][sub_case] += results[name][sub_case]\n",
    "    pickle.dump(joined_results, open(WORKING_DIR+'/RNN_results.joined.pickle', 'wb'))\n",
    "else:\n",
    "    joined_results = results\n",
    "\n",
    "# Display and save overall model results\n",
    "with open(WORKING_DIR+'/overall_accs.txt', 'w') as f:\n",
    "    for name in joined_results.keys():\n",
    "        if \"npi\" in name:\n",
    "            sents = analyze_npi_results(joined_results[name])\n",
    "            overall_gi, overall_iu, overall_gu = display_npi_results(name, sents)\n",
    "            f.write(name+\"(grammatical vs. intrusive): \"+str(overall_gi)+\"\\n\")\n",
    "            f.write(name+\"(intrusive vs. ungrammatical): \"+str(overall_iu)+\"\\n\")\n",
    "            f.write(name+\"(grammatical vs. ungrammatical): \"+str(overall_gu)+\"\\n\")\n",
    "        else:\n",
    "            sents = analyze_agrmt_results(joined_results[name])\n",
    "            overall = display_agrmt_results(name, sents)\n",
    "            f.write(name+\": \"+str(overall)+\"\\n\")\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
