{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Pipeline\n",
    "This is a simple but functional notebook which generates overall accuracy results for a given model. Below, please enter the path for your model .pt and .bin files and a directory where output should be saved. Then just run the notebook. Theoretically, that's it.\n",
    "\n",
    "Look for \"overall_accs.txt\" and \"rnn.output\" in your working directly. Those are the results.\n",
    "\n",
    "This code works as intended on my machine, but **please let me know if it breaks on yours**.\n",
    "\n",
    "This pipeline is heavily reliant on other functions scattered throughout the repo, so please be sure you haven't moved it anywhere else. It is also heavily reliant on the model structure and output files generated during training with our base code. It is finicky enough that if you have any issues, it's probably easiest to just ask me to fix it (or at least show me the error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USER INPUT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the directory where you would like final result files to be saved.\n",
    "# Then enter the path for your model .pt and .bin files. You must have both.\n",
    "WORKING_DIR = 'C:/Users/eweeding/Documents/MSE/DeepLearning/FinalProject'\n",
    "MODEL_PT = WORKING_DIR+'/model_mini.pt'\n",
    "MODEL_BIN = WORKING_DIR+'/model_mini.bin'\n",
    "\n",
    "# Set model type. For now, only single-task RNN is supported.\n",
    "MODEL_TYPE = 'RNN'\n",
    "\n",
    "# Set analysis mode. For now, only \"overall\" is supported.\n",
    "MODE = 'overall'\n",
    "\n",
    "### OPTIONAL ###\n",
    "# Other paths you may need or want to change depending on your setup:\n",
    "TEMPLATE_DIR = './EMNLP2018/templates' # Location of the paired sentence dataset\n",
    "OUTPUT_FILE = 'all_test_sents.txt'     # Name of .txt file which will contain all test sentences, saved in TEMPLATE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END USER INPUT #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Import a module within the base code\n",
    "sys.path.append(os.path.join(os.getcwd(), 'word-language-model'))\n",
    "import data\n",
    "\n",
    "# Check for GPU\n",
    "gpu_avail = torch.cuda.is_available()\n",
    "print('GPU available:', gpu_avail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate paired sentences\n",
    "This generates the default sentence templates for testing. These already exist in ./EMNLP2018/templates, but you may need to generate them for your system if Pickle is being dumb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case: obj_rel_across_anim\n",
      "case: obj_rel_within_anim\n",
      "case: obj_rel_across_inanim\n",
      "case: obj_rel_within_inanim\n",
      "case: subj_rel\n",
      "case: prep_anim\n",
      "case: prep_inanim\n",
      "case: obj_rel_no_comp_across_anim\n",
      "case: obj_rel_no_comp_within_anim\n",
      "case: obj_rel_no_comp_across_inanim\n",
      "case: obj_rel_no_comp_within_inanim\n",
      "case: simple_agrmt\n",
      "case: sent_comp\n",
      "case: vp_coord\n",
      "case: long_vp_coord\n",
      "case: reflexives_across\n",
      "case: simple_reflexives\n",
      "case: reflexive_sent_comp\n",
      "case: npi_across_anim\n",
      "case: npi_across_inanim\n",
      "case: simple_npi_anim\n",
      "case: simple_npi_inanim\n"
     ]
    }
   ],
   "source": [
    "%run ./src/make_templates.py $TEMPLATE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test model on the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity measures and other basic functions\n",
    "def get_entropy(o):\n",
    "    ## o should be a vector scoring possible classes\n",
    "    probs = nn.functional.softmax(o,dim=0)\n",
    "    logprobs = nn.functional.log_softmax(o,dim=0) #numerically more stable than two separate operations\n",
    "    return -1 * torch.sum(probs * logprobs)\n",
    "\n",
    "def get_surps(o):\n",
    "    ## o should be a vector scoring possible classes\n",
    "    logprobs = nn.functional.log_softmax(o,dim=0)\n",
    "    return -1 * logprobs\n",
    "\n",
    "def get_complexity_apply(o,t,sentid,tags=False):\n",
    "    ## Use apply() method\n",
    "    Hs = torch.squeeze(apply(get_entropy,o))\n",
    "    surps = apply(get_surps,o)\n",
    "    \n",
    "    for corpuspos,targ in enumerate(t):\n",
    "        if tags:\n",
    "            word = corpus.dictionary.idx2tag[int(targ)]\n",
    "        else:\n",
    "            word = corpus.dictionary.idx2word[int(targ)]\n",
    "        if word == '<eos>' or word == '<EOS>':\n",
    "            #don't output the complexity of EOS\n",
    "            continue\n",
    "        surp = surps[corpuspos][int(targ)]\n",
    "        \n",
    "        with open(WORKING_DIR+'/rnn.output', 'a') as f:\n",
    "            f.write('\\n' + str(word)+' '+str(sentid)+' '+str(corpuspos)+' '+str(len(word))+' '+str(float(surp))+' '+str(float(Hs[corpuspos])))\n",
    "\n",
    "def apply(func, M):\n",
    "    ## applies a function along a given dimension\n",
    "    tList = [func(m) for m in torch.unbind(M,dim=0) ]\n",
    "    res = torch.stack(tList)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for getting batches and evaluating on test data\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "def test_get_batch(source, evaluation=False):\n",
    "    if isinstance(source, tuple):\n",
    "        seq_len = len(source[0]) - 1\n",
    "        data = Variable(source[0][:seq_len], requires_grad=False)\n",
    "        target = Variable(source[1][:seq_len], requires_grad=False)\n",
    "    else:\n",
    "        seq_len = len(source) - 1\n",
    "        data = Variable(source[:seq_len], requires_grad=False)\n",
    "        target = Variable(source[1:1+seq_len].view(-1))\n",
    "    if gpu_avail:\n",
    "        return data.cuda(), target.cuda()\n",
    "    else:\n",
    "        return data, target\n",
    "\n",
    "def test_evaluate(test_lm_sentences, test_ccg_sentences, lm_data_source, ccg_data_source):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()    \n",
    "    \n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    \n",
    "    with open(WORKING_DIR+'/rnn.output', 'w') as f:\n",
    "        f.write('word sentid sentpos wlen surp entropy')\n",
    "    \n",
    "    for i in range(len(lm_data_source)+len(ccg_data_source)):\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f'{i} / {len(lm_data_source)} sentences')\n",
    "        \n",
    "        if i >= len(lm_data_source):\n",
    "            sent_ids = ccg_data_source[i-len(lm_data_source)]\n",
    "            sent = test_ccg_sentences[i-len(lm_data_source)]\n",
    "        else:\n",
    "            sent_ids = lm_data_source[i]\n",
    "            sent = test_lm_sentences[i]\n",
    "        \n",
    "        if gpu_avail:\n",
    "            sent_ids = sent_ids.cuda()\n",
    "            \n",
    "        hidden = model.init_hidden(1) # number of parallel sentences being processed\n",
    "        \n",
    "        data, targets = test_get_batch(sent_ids, evaluation=True)\n",
    "        data=data.unsqueeze(1) # only needed if there is just a single sentence being processed\n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        curr_loss = criterion(output_flat, targets).item()\n",
    "        total_loss += curr_loss\n",
    "\n",
    "        # output word-level complexity metrics\n",
    "        if i >= len(lm_data_source):\n",
    "            get_complexity_apply(output_flat,targets,i-len(lm_data_source),tags=True)\n",
    "        else:\n",
    "            get_complexity_apply(output_flat,targets,i)\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "\n",
    "    return total_loss / (len(lm_data_source)+len(ccg_data_source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"main\" function that coordinates testing\n",
    "def run_main(lm_data, save, save_lm_data, testfname):\n",
    "    \n",
    "    global corpus\n",
    "    corpus = data.SentenceCorpus(lm_data, False, save_lm_data, True, testfname=testfname)\n",
    "    \n",
    "    test_lm_sentences, test_lm_data = corpus.test_lm\n",
    "    test_ccg_sentences = []\n",
    "    test_ccg_data = []\n",
    "    \n",
    "    # Load the saved model\n",
    "    global model\n",
    "    if gpu_avail:\n",
    "        model = torch.load(save, map_location = 'cuda:0')\n",
    "    else:\n",
    "        model = torch.load(save, map_location = 'cpu')    \n",
    "    print('Your model is:')\n",
    "    print(model)\n",
    "\n",
    "    # Run on test data\n",
    "    test_loss = test_evaluate(test_lm_sentences, test_ccg_sentences, test_lm_data, test_ccg_data)\n",
    "    \n",
    "    print('=' * 89)\n",
    "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "    print('=' * 89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Writing tests...\n"
     ]
    }
   ],
   "source": [
    "from src.tester.TestWriter import TestWriter\n",
    "from src.template.TestCases import TestCase\n",
    "\n",
    "writer = TestWriter(TEMPLATE_DIR, OUTPUT_FILE)\n",
    "testcase = TestCase()\n",
    "tests = testcase.all_cases\n",
    "\n",
    "all_test_sents = {}\n",
    "for test_name in tests:\n",
    "    test_sents = pickle.load(open(TEMPLATE_DIR+\"/\"+test_name+\".pickle\", 'rb'))\n",
    "    all_test_sents[test_name] = test_sents\n",
    "\n",
    "writer.write_tests(all_test_sents, 'word')\n",
    "name_lengths = writer.name_lengths\n",
    "key_lengths = writer.key_lengths\n",
    "\n",
    "def score_rnn():\n",
    "    print(\"Scoring RNN...\")\n",
    "    with open(WORKING_DIR+'/rnn.output', 'r') as f:\n",
    "        all_scores = {}\n",
    "        first = False\n",
    "        score = 0.\n",
    "        sent = []\n",
    "        prev_sentid = -1\n",
    "        for line in f:\n",
    "            if not first:\n",
    "                first = True\n",
    "                continue\n",
    "            #print(line)\n",
    "            #print(line.strip())\n",
    "            if first and len(line.strip().split()) == 6 and \"torch.cuda\" not in line:\n",
    "                wrd, sentid, wrd_score = [line.strip().split()[i] for i in [0,1,4]]\n",
    "                score = -1 * float(wrd_score) # multiply by -1 to turn surps back into logprobs\n",
    "                sent.append((wrd, score))\n",
    "                if wrd == \".\":\n",
    "                    name_found = False\n",
    "                    for (k1,v1) in sorted(name_lengths.items(), key=operator.itemgetter(1)):\n",
    "                        if float(sentid) < v1 and not name_found:\n",
    "                            name_found = True\n",
    "                            if k1 not in all_scores:\n",
    "                                all_scores[k1] = {}\n",
    "                            key_found = False\n",
    "                            for (k2,v2) in sorted(key_lengths[k1].items(), key=operator.itemgetter(1)):\n",
    "                                if int(sentid) <  v2 and not key_found:\n",
    "                                    key_found = True\n",
    "                                    if k2 not in all_scores[k1]:\n",
    "                                        all_scores[k1][k2] = []\n",
    "                                    all_scores[k1][k2].append(sent)\n",
    "                    sent = []\n",
    "                    if float(sentid) != prev_sentid+1:\n",
    "                        logging.info(\"Error at sents \"+sentid+\" and \"+prev_sentid)\n",
    "                    prev_sentid = float(sentid)\n",
    "    return all_scores\n",
    "\n",
    "def test_LM():      \n",
    "    print(\"Testing RNN...\")\n",
    "    run_main(TEMPLATE_DIR, MODEL_PT, MODEL_BIN, OUTPUT_FILE)\n",
    "    results = score_rnn()\n",
    "    with open(WORKING_DIR+'/'+MODEL_TYPE+'_results.pickle', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RNN...\n",
      "Your model is:\n",
      "RNNModel(\n",
      "  (drop): Dropout(p=0.2, inplace=False)\n",
      "  (encoder): Embedding(50001, 200)\n",
      "  (rnn): LSTM(200, 200, num_layers=2, dropout=0.2)\n",
      "  (decoder): Linear(in_features=200, out_features=50001, bias=True)\n",
      ")\n",
      "0 / 11916 sentences\n",
      "1000 / 11916 sentences\n",
      "2000 / 11916 sentences\n",
      "3000 / 11916 sentences\n",
      "4000 / 11916 sentences\n",
      "5000 / 11916 sentences\n",
      "6000 / 11916 sentences\n",
      "7000 / 11916 sentences\n",
      "8000 / 11916 sentences\n",
      "9000 / 11916 sentences\n",
      "10000 / 11916 sentences\n",
      "11000 / 11916 sentences\n",
      "=========================================================================================\n",
      "| End of training | test loss  7.12 | test ppl  1236.42\n",
      "=========================================================================================\n",
      "Scoring RNN...\n"
     ]
    }
   ],
   "source": [
    "# Generate testing output\n",
    "test_LM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj_rel_across_anim\n",
      "obj_rel_within_anim\n",
      "obj_rel_across_inanim\n",
      "obj_rel_within_inanim\n",
      "subj_rel\n",
      "prep_anim\n",
      "prep_inanim\n",
      "obj_rel_no_comp_across_anim\n",
      "obj_rel_no_comp_within_anim\n",
      "obj_rel_no_comp_across_inanim\n",
      "obj_rel_no_comp_within_inanim\n",
      "simple_agrmt\n",
      "sent_comp\n",
      "vp_coord\n",
      "long_vp_coord\n",
      "reflexives_across\n",
      "simple_reflexives\n",
      "reflexive_sent_comp\n",
      "npi_across_anim\n",
      "npi_across_inanim\n",
      "simple_npi_anim\n",
      "simple_npi_inanim\n"
     ]
    }
   ],
   "source": [
    "from src.template.TestCases import TestCase\n",
    "\n",
    "testcase = TestCase()\n",
    "tests = testcase.all_cases\n",
    "\n",
    "results = pickle.load(open(WORKING_DIR+'/RNN_results.pickle', 'rb'))\n",
    "\n",
    "joined_results = {}\n",
    "for name in tests:\n",
    "    print(name)\n",
    "    if 'anim' in name:\n",
    "        new_name = '_'.join(name.split(\"_\")[:-1])\n",
    "    else:\n",
    "        new_name = name\n",
    "    for sub_case in results[name]:\n",
    "        if new_name not in joined_results:\n",
    "            joined_results[new_name] = {}\n",
    "        if sub_case not in joined_results[new_name]:\n",
    "            joined_results[new_name][sub_case] = []\n",
    "        joined_results[new_name][sub_case] += results[name][sub_case]\n",
    "# dump joined results to .pickle file\n",
    "pickle.dump(joined_results, open(WORKING_DIR+'/RNN_results.joined.pickle', 'wb'))\n",
    "\n",
    "def is_more_probable(sent_a, sent_b):\n",
    "    if len(sent_a) != len(sent_b) and args.unit_type == 'word':\n",
    "        logging.info(\"ERROR: Mismatch in sentence lengths: (1) \",sent_a, \" vs (2) \",sent_b)\n",
    "    return sum([sent_a[i][1] for i in range(len(sent_a))]) > sum([sent_b[i][1] for i in range(len(sent_b))])\n",
    "\n",
    "def analyze_agrmt_results(results):\n",
    "    correct_sents = {}\n",
    "    incorrect_sents = {}\n",
    "    for case in results.keys():\n",
    "        correct_sents[case] = []\n",
    "        incorrect_sents[case] = []\n",
    "        for i in range(0,len(results[case]),2):\n",
    "            grammatical = results[case][i]\n",
    "            ungrammatical = results[case][i+1]\n",
    "            if is_more_probable(grammatical, ungrammatical):\n",
    "                correct_sents[case].append((grammatical, ungrammatical))\n",
    "            else:\n",
    "                incorrect_sents[case].append((grammatical, ungrammatical))\n",
    "    return correct_sents, incorrect_sents\n",
    "\n",
    "def display_agrmt_results(name, sents):\n",
    "    # print case-by-case accuracies\n",
    "    correct_sents, incorrect_sents = sents\n",
    "    overall_correct = 0.\n",
    "    total = 0.\n",
    "    strings = {}\n",
    "    case_accs = {}\n",
    "    for case in correct_sents.keys():\n",
    "        overall_correct += len(correct_sents[case])\n",
    "        total += len(correct_sents[case]) + len(incorrect_sents[case])\n",
    "    return float(overall_correct)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing results for obj_rel_across\n",
      "Analyzing results for obj_rel_within\n",
      "Analyzing results for subj_rel\n",
      "Analyzing results for prep\n",
      "Analyzing results for obj_rel_no_comp_across\n",
      "Analyzing results for obj_rel_no_comp_within\n",
      "Analyzing results for simple_agrmt\n",
      "Analyzing results for sent_comp\n",
      "Analyzing results for vp_coord\n",
      "Analyzing results for long_vp_coord\n",
      "Analyzing results for reflexives_across\n",
      "Analyzing results for simple_reflexives\n",
      "Analyzing results for reflexive_sent_comp\n"
     ]
    }
   ],
   "source": [
    "with open(WORKING_DIR+'/overall_accs.txt', 'w') as f:\n",
    "    for name in joined_results.keys():\n",
    "        if \"npi\" in name:\n",
    "            continue\n",
    "        else:\n",
    "            print(f'Analyzing results for {name}')\n",
    "            sents = analyze_agrmt_results(joined_results[name])\n",
    "            overall = display_agrmt_results(name, sents)\n",
    "            f.write(name+\": \"+str(overall)+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
